## 目的
エージェントによる最終回答の精度を評価するための方法を模索する．

## 検証手法
* ２つの文章の意味的類似度をChatGPTに評価させる．
* 評価は5段階評価とする．
  - 評価:5
    * 文章が完全に同一
  - 評価:4
    * 文章の一部が異なるが、文意は同一
    * AWS CLIのパラメータとなるような値が完全一致
  - 評価:3
    * 評価元の文の意味に過不足がある
    * AWS CLIのパラメータとなるような値が複数ある場合は、半分以上一致
    * AWS CLIのパラメータとなるような値が単一の場合は値が完全一致
  - 評価:2
    * 評価元の文に同一の話題が述べられているが、意味が不一致
    * AWS CLIのパラメータとなるような値が1～4割一致
    * AWS CLIのパラメータとなるような値が単一の場合は値が不一致
  - 評価:1
    * 文意が全く異なる

## 検証結果
* [結果レポート](./results/evaluation/result_evaluation_rule.md)

## まとめと感想
* ルールを設定することである程度の評価品質を担保できることが分かった．

* 評価ルールを入れない場合、「ストレージサイズは８GB」と「ストレージサイズは１GB」を高く評価
  - 理由は「ストレージサイズが異なるだけで文意はほぼ一緒だから」
  - ユーザー的にはそれは低く評価してほしい…
  - AWS CLIのパラメータを見るようルールに明記するに至った．

* 「エンジニアは５名」と「エンジニアは４名」のペアを高く評価
  - プロジェクトの係数的部分もルールを入れておけば正確に答えられそう
  - 逆に言えば、ルールを指定する場合は、柔軟性が失われることもある
  - AIの回答パターンは無限大に存在するので、評価法をどこで妥協するのかは議論が必要

* この方式では以下の特性がある
  - 人間の模範解答を準備する必要がある
  - 評価ルールの具体度／抽象度は評価ルール数とトレードオフ
